{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kode-git/Copernicus-river-discharges/blob/main/Initial_Exploratory_Spatial_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./src/copernicus-logo.png\"><span style=\"margin-left: 40px\"></span><img src=\"./src/cds-logo.jpeg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZFMn0o7-HzB"
      },
      "source": [
        "# Initial Exploratory Spatial Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the initial exploratory of spatial data, we will focus on analyzing the data format. This notebook is essential for data management, and it is an introduction to the data aggregation phase due to the representations of metadata. Furthermore, we can specify the internal structure of a NETCDF4 file representing a well-known N-dimensional collection. Information mined from the data structures will be vital for critical decisions on the next steps. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDJZn5h8jnA7"
      },
      "source": [
        "## Library Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REN9dS_FnnK0",
        "outputId": "17c24f14-90da-4dbe-d02f-134c2a4a6b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xarray in /usr/local/lib/python3.9/site-packages (2022.3.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.9/site-packages (from xarray) (1.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from xarray) (21.3)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.9/site-packages (from xarray) (1.22.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->xarray) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.1->xarray) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.1->xarray) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1->xarray) (1.16.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.9/site-packages (1.5.8)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.9/site-packages (2022.5.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.9/site-packages (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.9/site-packages (from netCDF4) (1.22.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.9/site-packages (from netCDF4) (1.6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/site-packages (from dask) (2022.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/site-packages (from dask) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from dask) (21.3)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/site-packages (from dask) (1.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/site-packages (from dask) (2.0.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/site-packages (from dask) (0.11.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->dask) (3.0.8)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.9/site-packages (from partd>=0.3.10->dask) (1.0.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.9/site-packages (0.10.2)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.9/site-packages (from geopandas) (3.3.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.9/site-packages (from geopandas) (1.4.2)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.9/site-packages (from geopandas) (1.8.21)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.9/site-packages (from geopandas) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (60.10.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (1.22.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (2022.1)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: cdsapi in /usr/local/lib/python3.9/site-packages (0.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from cdsapi) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.9/site-packages (from cdsapi) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.5.0->cdsapi) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.5.0->cdsapi) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests>=2.5.0->cdsapi) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.5.0->cdsapi) (1.26.9)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installation of dependencies for remote notebook (Jupyter or Google Colab)\n",
        "# !pip install xarray \n",
        "# !pip install netCDF4 dask bottleneck\n",
        "# !pip install pandas\n",
        "# !pip install geopandas\n",
        "# !pip install cdsapi\n",
        "\n",
        "# installation of dependencies for local notebook\n",
        "%pip install xarray \n",
        "%pip install netCDF4 dask bottleneck\n",
        "%pip install pandas\n",
        "%pip install geopandas\n",
        "%pip install cdsapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NEr_60kVbUHS"
      },
      "outputs": [],
      "source": [
        "# collections libraries\n",
        "import netCDF4 as nc4\n",
        "from netCDF4 import Dataset\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "\n",
        "# file management\n",
        "from glob import glob\n",
        "\n",
        "# utilities \n",
        "from datetime import datetime as dt\n",
        "\n",
        "# remove the comment on it only if use google colaboratory\n",
        "# colab \n",
        "# from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAzT_trZgcFB",
        "outputId": "3aba4905-28d5-4fc8-8512-9786c320e555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# remove the comment on it only if use google colaboratory\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cf6MRy0rFmq"
      },
      "source": [
        "## Initial Exploratory on Spatial River Discharge Data (RDH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RDH is the first of the two datasets we will use during the project. The original name is \"River discharge and related historical data from the European Flood Awareness System.\" It represents a geographical distribution of river discharges on the whole European surface in $m^3/s$. The amount of data on this dataset overextends the real and not nullable information. The original dataset is unique; we have been split it due to the huge dimensions of files and the data limits for downloading on the Climate Data Store (CDS). During the exploratory, we will discuss only a subset of information for a time interval of 2 years inside the 2021-2022 events due to the expensive time spent on big data management and because data don't change metadata and layout over time; it is a good practice specified that the amount of data i two years available for the bound 2021 and 2022 isn't complete because the analysis is on the present and past data. We will match geographical locations with precipitations and temperatures after possible data shifting to match the measurements during the data aggregation phase. A complete and well-documented data source should be available on the Climate Data Store (CDS) by clicking on the following <a href=\"https://cds.climate.copernicus.eu/cdsapp#!/dataset/efas-historical?tab=overview\">link</a>. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeFX3i9IyUj4",
        "outputId": "2226bbbd-aeb9-4ab1-d7ef-6d52af8ec6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'y': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 y(y)\n",
            "    _FillValue: nan\n",
            "    units: Meter\n",
            "    long_name: Y coordinate of projection\n",
            "    standard_name: projection_Y_coordinate\n",
            "unlimited dimensions: \n",
            "current shape = (950,)\n",
            "filling on, 'x': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 x(x)\n",
            "    _FillValue: nan\n",
            "    units: Meter\n",
            "    long_name: x coordinate of projection\n",
            "    standard_name: projection_x_coordinate\n",
            "unlimited dimensions: \n",
            "current shape = (1000,)\n",
            "filling on, 'time': <class 'netCDF4._netCDF4.Variable'>\n",
            "int64 time(time)\n",
            "    long_name: initial time of forecast\n",
            "    standard_name: forecast_reference_time\n",
            "    units: seconds since 1970-01-01\n",
            "    calendar: proleptic_gregorian\n",
            "unlimited dimensions: \n",
            "current shape = (498,)\n",
            "filling on, default _FillValue of -9223372036854775806 used, 'step': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 step()\n",
            "    _FillValue: nan\n",
            "    long_name: time since forecast_reference_time\n",
            "    standard_name: forecast_period\n",
            "    units: hours\n",
            "unlimited dimensions: \n",
            "current shape = ()\n",
            "filling on, 'surface': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 surface()\n",
            "    _FillValue: nan\n",
            "    long_name: original GRIB coordinate for key: level(surface)\n",
            "    units: 1\n",
            "unlimited dimensions: \n",
            "current shape = ()\n",
            "filling on, 'latitude': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 latitude(y, x)\n",
            "    _FillValue: nan\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    long_name: latitude\n",
            "    standard_name: latitude\n",
            "    units: degrees_north\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on, 'longitude': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 longitude(y, x)\n",
            "    _FillValue: nan\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    long_name: longitude\n",
            "    standard_name: longitude\n",
            "    units: degrees_east\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on, 'valid_time': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 valid_time(time)\n",
            "    _FillValue: nan\n",
            "    standard_name: time\n",
            "    long_name: time\n",
            "    units: seconds since 1970-01-01\n",
            "    calendar: proleptic_gregorian\n",
            "unlimited dimensions: \n",
            "current shape = (498,)\n",
            "filling on, 'dis06': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 dis06(time, y, x)\n",
            "    _FillValue: nan\n",
            "    GRIB_paramId: 240023\n",
            "    GRIB_dataType: sfo\n",
            "    GRIB_numberOfPoints: 950000\n",
            "    GRIB_typeOfLevel: surface\n",
            "    GRIB_stepUnits: 1\n",
            "    GRIB_stepType: avg\n",
            "    GRIB_gridType: lambert_azimuthal_equal_area\n",
            "    GRIB_NV: 0\n",
            "    GRIB_cfName: unknown\n",
            "    GRIB_cfVarName: dis06\n",
            "    GRIB_gridDefinitionDescription: Lambert azimuthal equal area projection\n",
            "    GRIB_missingValue: 9999\n",
            "    GRIB_name: Mean discharge in the last 6 hours\n",
            "    GRIB_shortName: dis06\n",
            "    GRIB_units: m**3 s**-1\n",
            "    long_name: Mean discharge in the last 6 hours\n",
            "    units: m**3 s**-1\n",
            "    standard_name: unknown\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    coordinates: time step surface latitude longitude valid_time\n",
            "unlimited dimensions: \n",
            "current shape = (498, 950, 1000)\n",
            "filling on, 'lambert_azimuthal_equal_area': <class 'netCDF4._netCDF4.Variable'>\n",
            "int32 lambert_azimuthal_equal_area()\n",
            "    semi_major_axis: 6378137.0\n",
            "    EPSG_code: EPSG:3035\n",
            "    latitude_of_projection_origin: 52.0\n",
            "    inverse_flattening: 298.257223563\n",
            "    longitude_of_projection_origin: 10.0\n",
            "    proj4_params: +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\n",
            "    false_northing: 3210000.0\n",
            "    grid_mapping_name: lambert_azimuthal_equal_area\n",
            "    false_easting: 4321000.0\n",
            "    coordinates: step surface\n",
            "unlimited dimensions: \n",
            "current shape = ()\n",
            "filling on, default _FillValue of -2147483647 used, 'land_binary_mask': <class 'netCDF4._netCDF4.Variable'>\n",
            "int8 land_binary_mask(y, x)\n",
            "    standard_name: land_binary_mask\n",
            "    long_name: masking_area\n",
            "    units: 1\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "    coordinates: longitude latitude step surface\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on, default _FillValue of -127 ignored, 'upArea': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 upArea(y, x)\n",
            "    _FillValue: nan\n",
            "    least_significant_digit: 2\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    long_name: upstream_area\n",
            "    standard_name: upstream\n",
            "    units: M2\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "    cell_methods: time: mean\n",
            "    coordinates: longitude latitude step surface\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on}\n"
          ]
        }
      ],
      "source": [
        "nc_dis = Dataset(\"samples/rdh-2021-2022-6.nc\", \"r\", \"NETCDF4\" )\n",
        "print(nc_dis.variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KVI8N0Y_g3S"
      },
      "source": [
        "### Data Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpFxXFDSbKAx",
        "outputId": "c1079412-c01c-4126-9ea9-7de2fd7b0c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables:\n",
            "River Discharges: dict_keys(['y', 'x', 'time', 'step', 'surface', 'latitude', 'longitude', 'valid_time', 'dis06', 'lambert_azimuthal_equal_area', 'land_binary_mask', 'upArea'])\n",
            "----------\n",
            "Dimensions:\n",
            "River Discharges dict_keys(['y', 'x', 'time'])\n"
          ]
        }
      ],
      "source": [
        "print('Variables:')\n",
        "print(\"River Discharges: {}\".format(nc_dis.variables.keys()))\n",
        "print('-'*10)\n",
        "print(\"Dimensions:\")\n",
        "print(\"River Discharges {}\".format(nc_dis.dimensions.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY9WvCUHbQ6K"
      },
      "source": [
        "We can find values corresponding to the keys of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Y7r_g6bQFW",
        "outputId": "a98644cf-f782-4fe1-c451-ca17639bb43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discharge variables metadata:\n",
            "<class 'netCDF4._netCDF4.Variable'>\n",
            "float32 dis06(time, y, x)\n",
            "    _FillValue: nan\n",
            "    GRIB_paramId: 240023\n",
            "    GRIB_dataType: sfo\n",
            "    GRIB_numberOfPoints: 950000\n",
            "    GRIB_typeOfLevel: surface\n",
            "    GRIB_stepUnits: 1\n",
            "    GRIB_stepType: avg\n",
            "    GRIB_gridType: lambert_azimuthal_equal_area\n",
            "    GRIB_NV: 0\n",
            "    GRIB_cfName: unknown\n",
            "    GRIB_cfVarName: dis06\n",
            "    GRIB_gridDefinitionDescription: Lambert azimuthal equal area projection\n",
            "    GRIB_missingValue: 9999\n",
            "    GRIB_name: Mean discharge in the last 6 hours\n",
            "    GRIB_shortName: dis06\n",
            "    GRIB_units: m**3 s**-1\n",
            "    long_name: Mean discharge in the last 6 hours\n",
            "    units: m**3 s**-1\n",
            "    standard_name: unknown\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    coordinates: time step surface latitude longitude valid_time\n",
            "unlimited dimensions: \n",
            "current shape = (498, 950, 1000)\n",
            "filling on\n"
          ]
        }
      ],
      "source": [
        "dis = nc_dis.variables['dis06']\n",
        "print(\"Discharge variables metadata:\\n{}\".format(dis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV22DZEEbr3w"
      },
      "source": [
        "The discharge value is an integer value formed by 3 dimensions - $[time, x, y]$. We can also get some other information about metadata like coordinates, common names, and units of semantic measurement, and we can also check their size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAnZPFQGco7q",
        "outputId": "a648227e-885a-4acc-bfd5-33f8e75a2bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Items on dimensions: ('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 950)\n",
            "Items on dimensions: ('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 1000)\n",
            "Items on dimensions: ('time', <class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 498)\n"
          ]
        }
      ],
      "source": [
        "for d in nc_dis.dimensions.items():\n",
        "  print(\"Items on dimensions: {}\".format(d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is essential to specify that $(x,y)$ is a projection of the point in a geographical environment, and they do not represent any coordinates on the final data. Projection points are the total size of points in a geographical map $(N, M)$ and not geographical coordinates of points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdL5JsU1dNvr",
        "outputId": "cc41c161-34e2-4e3c-c31c-974da492c46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "River Discharge dimensions: ('time', 'y', 'x')\n"
          ]
        }
      ],
      "source": [
        "print(\"River Discharge dimensions: {}\".format(dis.dimensions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0rwggtZdRHu"
      },
      "source": [
        "Printing the dimensions of the discharge variable, we can check how it is formed to confirm our first assumption. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tanq8ZmYdP8T",
        "outputId": "09fd6a2d-71b9-49f7-8555-e3bab11ca99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total shape: (498, 950, 1000)\n"
          ]
        }
      ],
      "source": [
        "print('Total shape: {}'.format(dis.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYY8XgmSde01"
      },
      "source": [
        "Similarly, we can also inspect the variables associated with each dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owBgScFDdZZ2",
        "outputId": "21935d3a-0ff8-47c0-c465-6a14a1032de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time variables :-> <class 'netCDF4._netCDF4.Variable'>\n",
            "int64 time(time)\n",
            "    long_name: initial time of forecast\n",
            "    standard_name: forecast_reference_time\n",
            "    units: seconds since 1970-01-01\n",
            "    calendar: proleptic_gregorian\n",
            "unlimited dimensions: \n",
            "current shape = (498,)\n",
            "filling on, default _FillValue of -9223372036854775806 used\n",
            "X coordinate :-> <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 x(x)\n",
            "    _FillValue: nan\n",
            "    units: Meter\n",
            "    long_name: x coordinate of projection\n",
            "    standard_name: projection_x_coordinate\n",
            "unlimited dimensions: \n",
            "current shape = (1000,)\n",
            "filling on\n",
            "Y coordinate :-> <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 y(y)\n",
            "    _FillValue: nan\n",
            "    units: Meter\n",
            "    long_name: Y coordinate of projection\n",
            "    standard_name: projection_Y_coordinate\n",
            "unlimited dimensions: \n",
            "current shape = (950,)\n",
            "filling on\n"
          ]
        }
      ],
      "source": [
        "time = nc_dis.variables['time']\n",
        "x,y = nc_dis.variables['x'], nc.variables['y']\n",
        "print(\"Time variables :->\", time)\n",
        "print(\"X coordinate :->\", x)\n",
        "print(\"Y coordinate :->\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cBUFQ9pd3pJ"
      },
      "source": [
        "Here, we obtained some information about each of the three dimensions. The time is related to the initial moment of the forecast of the discharges. Meanwhile, x and y are the coordinates in meters of the geographical projection, and the dimensions are in 1D. So, we can access it directly as a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL5hzn54d03Q",
        "outputId": "65c79cc1-9c6c-43f5-de51-d8f0e3e21369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 times: [1609502400 1609588800 1609675200 1609761600 1609848000 1609934400\n",
            " 1610020800 1610107200 1610193600 1610280000 1610366400 1610452800\n",
            " 1610539200 1610625600 1610712000 1610798400 1610884800 1610971200\n",
            " 1611057600 1611144000]\n",
            "Shape of time: (498,)\n"
          ]
        }
      ],
      "source": [
        "tm = time[:]\n",
        "print(\"First 20 times: {}\".format(tm[0:20]))\n",
        "print(\"Shape of time: {}\".format(time.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CTxGWsNey49"
      },
      "source": [
        "This property is similar to other dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YehZKh0ce3t0",
        "outputId": "d5175cc9-55c5-44aa-bbb8-9bcab07d9a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 x: [2502500. 2507500. 2512500. 2517500. 2522500. 2527500. 2532500. 2537500.\n",
            " 2542500. 2547500.]\n",
            "First 20 y: [5497500. 5492500. 5487500. 5482500. 5477500. 5472500. 5467500. 5462500.\n",
            " 5457500. 5452500.]\n",
            "Shape x : (1000,); shape y : (950,)\n"
          ]
        }
      ],
      "source": [
        "X = x[:]\n",
        "Y = y[:]\n",
        "print(\"First 20 x: {}\".format(X[0:10]))\n",
        "print(\"First 20 y: {}\".format(Y[0:10]))\n",
        "print('Shape x : {}; shape y : {}'.format(X.shape, Y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdTaj-J8h4e6"
      },
      "source": [
        "$x$ and $y$ are not the geographical coordinates related to the river discharges but only for the 2D graphical projection. In the starting snippet, we analyzed data structure, and we also saw a geographical reference for real coordinates: latitude and longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-LsEM72fKm1",
        "outputId": "8a309e47-c9b7-46d9-8698-b17b3c505390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latitude :-> <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 latitude(y, x)\n",
            "    _FillValue: nan\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    long_name: latitude\n",
            "    standard_name: latitude\n",
            "    units: degrees_north\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on\n",
            "Longitude :-> <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 longitude(y, x)\n",
            "    _FillValue: nan\n",
            "    grid_mapping: lambert_azimuthal_equal_area\n",
            "    long_name: longitude\n",
            "    standard_name: longitude\n",
            "    units: degrees_east\n",
            "    esri_pe_string: PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"false_easting\",4321000.0],PARAMETER[\"false_northing\",3210000.0],PARAMETER[\"central_meridian\",10.0],PARAMETER[\"latitude_of_origin\",52.0],UNIT[\"Meter\",1.0]]\n",
            "unlimited dimensions: \n",
            "current shape = (950, 1000)\n",
            "filling on\n",
            "Latitude values:\n",
            "[[66.98215  67.0096   67.03701  ... 58.318253 58.282505 58.24674 ]\n",
            " [66.94809  66.9755   67.00287  ... 58.294014 58.2583   58.22256 ]\n",
            " [66.913994 66.94137  66.9687   ... 58.269733 58.234047 58.198338]\n",
            " ...\n",
            " [66.742966 66.77013  66.79724  ... 58.1476   58.11206  58.076504]\n",
            " [66.70864  66.73576  66.76283  ... 58.12303  58.087524 58.051994]\n",
            " [66.674286 66.70136  66.72839  ... 58.098415 58.06294  58.02744 ]]\n",
            "Longitude values:\n",
            "[[-35.034023 -34.944263 -34.85432  ...  73.82979   73.88378   73.937675]\n",
            " [-34.958313 -34.868584 -34.77867  ...  73.759285  73.81332   73.86726 ]\n",
            " [-34.88283  -34.793133 -34.703255 ...  73.6889    73.74297   73.79696 ]\n",
            " ...\n",
            " [-34.508877 -34.41934  -34.329624 ...  73.3387    73.39298   73.44718 ]\n",
            " [-34.43477  -34.345264 -34.255585 ...  73.269005  73.32333   73.37757 ]\n",
            " [-34.360886 -34.271416 -34.18177  ...  73.19943   73.2538    73.308075]]\n"
          ]
        }
      ],
      "source": [
        "lat, lon = nc_dis.variables['latitude'], nc_dis.variables['longitude']\n",
        "print(\"Latitude :-> {}\".format(lat))\n",
        "print(\"Longitude :-> {}\".format(lon))\n",
        "print('Latitude values:')\n",
        "print(lat[:10])\n",
        "print('Longitude values:')\n",
        "print(lon[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8uVGj70qCPo"
      },
      "source": [
        "## Initial Exploratory on Spatial Temperatures and Precipitations Data (TPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TPI is the second of the two used datasets. The original name is \"Temperature and precipitation gridded data for global and regional domains derived from in-situ and satellite observations\". It represents a geographical measurement of precipitations and temperatures in the European area over time. The original dataset is unique; we have been split it due to the vast dimensions of files and the data limits for downloading on the Climate Data Store (CDS). During the exploratory, we will discuss only a subset of information for a time interval of 1 year inside the 2021 events due to the expensive time spent on big data management and because data do not change metadata and layout over time. We can download the data source following the <a href=\"https://cds.climate.copernicus.eu/cdsapp#!/dataset/insitu-gridded-observations-global-and-regional?tab=overview\">link</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "nc_temp = Dataset(\"samples/tpi-temp-2021-10.nc\", \"r\", \"NETCDF4\")\n",
        "nc_prec = Dataset(\"samples/tpi-prec-2021-10.nc\", \"r\", \"NETCDF4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperatures metadata:\n",
            "<class 'netCDF4._netCDF4.Dataset'>\n",
            "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
            "    CDI: Climate Data Interface version 1.9.6 (http://mpimet.mpg.de/cdi)\n",
            "    Conventions: CF-1.6\n",
            "    CDO: Climate Data Operators version 1.9.6 (http://mpimet.mpg.de/cdo)\n",
            "    creation_date: 2020-02-14T09:31:29ZCET+0100\n",
            "    NCO: netCDF Operators version 4.7.7 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n",
            "    acknowledgements: This work was performed within Copernicus Climate Change Service - C3S_424_SMHI, https://climate.copernicus.eu/operational-service-water-sector, on behalf of ECMWF and EU.\n",
            "    contact: Hydro.fou@smhi.se\n",
            "    data_quality: Testing of EURO-CORDEX data performed by ESGF nodes. In the contract C3S_424_SMHI additional tests were performed during the bias adjustment and the prodcution of the CII.\n",
            "    domain: EUR-11\n",
            "    institution: SMHI, www.smhi.se\n",
            "    invar_bc_institution: Swedish Meteorological and Hydrological Institute\n",
            "    invar_bc_method: TimescaleBC, description in deliverable C3S_D424.SMHI.1.3b\n",
            "    invar_bc_method_id: TimescaleBC v1.02\n",
            "    invar_bc_observation: EFAS-Meteo, https://ec.europa.eu/jrc/en/publication/eur-scientific-and-technical-research-reports/efas-meteo-european-daily-high-resolution-gridded-meteorological-data-set-1990-2011\n",
            "    invar_bc_observation_id: EFAS-Meteo\n",
            "    invar_bc_period: 1990-2018\n",
            "    keywords: temperature, air\n",
            "    license: Copernicus License V1.2\n",
            "    output_frequency: daily\n",
            "    project_id: C3S_424_SMHI\n",
            "    references: Ntegeka et al., 2013 (http://dx.doi.org/10.2788/51262)\n",
            "    source: The RCM data originate from EURO-CORDEX (Coordinated Downscaling Experiment - European Domain, EUR-11) https://euro-cordex.net/.\n",
            "    summary: Daily mean air temperature at 2m height from an ensmeble of EURO-CORDEX EUR11. The variable is bias adjusted with theTimescaleBC method and using the EFAS-Meteo reference dataset. \n",
            "    tittle: Daily mean air Temperature at 2m height\n",
            "    invar_rcm_model_driver: MPI-M-MPI-ESM-LR\n",
            "    invar_experiment_id: rcp26\n",
            "    invar_realisation_id: r1i1p1\n",
            "    invar_rcm_model_id: MPI-CSC-REMO2009\n",
            "    time_period: 2021\n",
            "    history: \n",
            "    dimensions(sizes): time(365), x(1000), y(950)\n",
            "    variables(dimensions): float64 time(time), float32 lon(y, x), float32 lat(y, x), float32 tasAdjust(time, y, x), float64 height()\n",
            "    groups: \n"
          ]
        }
      ],
      "source": [
        "print(\"Temperatures metadata:\\n{}\".format(nc_temp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the dataset has the exact dimensions of the RDH. It is good information because we can focus directly on our statistical hypothesis after the data aggregation if data references are the same for each point. This process should be much easier than procedures with clustering and geographical approximations like k-means techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precipitations metadata:\n",
            "<class 'netCDF4._netCDF4.Dataset'>\n",
            "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
            "    CDI: Climate Data Interface version 1.9.6 (http://mpimet.mpg.de/cdi)\n",
            "    Conventions: CF-1.6\n",
            "    CDO: Climate Data Operators version 1.9.6 (http://mpimet.mpg.de/cdo)\n",
            "    creation_date: 2020-02-14T09:03:09ZCET+0100\n",
            "    NCO: netCDF Operators version 4.7.7 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n",
            "    acknowledgements: This work was performed within Copernicus Climate Change Service - C3S_424_SMHI, https://climate.copernicus.eu/operational-service-water-sector, on behalf of ECMWF and EU.\n",
            "    contact: Hydro.fou@smhi.se\n",
            "    domain: EUR-11\n",
            "    institution: SMHI, www.smhi.se\n",
            "    invar_bc_institution: Swedish Meteorological and Hydrological Institute\n",
            "    invar_bc_method: TimescaleBC, description in deliverable C3S_D424.SMHI.1.3b\n",
            "    invar_bc_method_id: TimescaleBC v1.02\n",
            "    invar_bc_observation: EFAS-Meteo, https://ec.europa.eu/jrc/en/publication/eur-scientific-and-technical-research-reports/efas-meteo-european-daily-high-resolution-gridded-meteorological-data-set-1990-2011\n",
            "    invar_bc_observation_id: EFAS-Meteo\n",
            "    invar_bc_period: 1990-2018\n",
            "    keywords: precipitation\n",
            "    license: Copernicus License V1.2\n",
            "    output_frequency: daily\n",
            "    project_id: C3S_424_SMHI\n",
            "    references: Ntegeka et al., 2013 (http://dx.doi.org/10.2788/51262)\n",
            "    source: The RCM data originate from EURO-CORDEX (Coordinated Downscaling Experiment - European Domain, EUR-11) https://euro-cordex.net/.\n",
            "    summary: Daily precipitation from an ensmeble of EURO-CORDEX EUR11. The variable is bias adjusted with the TimescaleBC method and using the EFAS-Meteo reference dataset. \n",
            "    tittle: Daily Precipitation \n",
            "    invar_rcm_model_driver: MPI-M-MPI-ESM-LR\n",
            "    invar_experiment_id: rcp26\n",
            "    invar_realisation_id: r1i1p1\n",
            "    invar_rcm_model_id: MPI-CSC-REMO2009\n",
            "    time_period: 2021\n",
            "    history: \n",
            "    data_quality: Testing of EURO-CORDEX data performed by ESGF nodes. In the contract C3S_424_SMHI additional tests were performed during the bias adjustment and the prodcution of the CII. Limitations in the bias adjustment method can lead to rare cases with extreme precipitation anomalies.\n",
            "    dimensions(sizes): time(365), x(1000), y(950)\n",
            "    variables(dimensions): float64 time(time), float32 lon(y, x), float32 lat(y, x), float32 prAdjust(time, y, x)\n",
            "    groups: \n"
          ]
        }
      ],
      "source": [
        "print(\"Precipitations metadata:\\n{}\".format(nc_prec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables:\n",
            "Temperature: dict_keys(['time', 'lon', 'lat', 'tasAdjust', 'height'])\n",
            "Precipitations: dict_keys(['time', 'lon', 'lat', 'prAdjust'])\n",
            "----------\n",
            "Dimensions:\n",
            "Temperature: dict_keys(['time', 'x', 'y'])\n",
            "Precipitations: dict_keys(['time', 'x', 'y'])\n"
          ]
        }
      ],
      "source": [
        "print('Variables:')\n",
        "print(\"Temperature: {}\".format(nc_temp.variables.keys()) )\n",
        "print(\"Precipitations: {}\".format(nc_prec.variables.keys()))\n",
        "print('-'*10)\n",
        "print(\"Dimensions:\")\n",
        "print(\"Temperature: {}\".format(nc_temp.dimensions.keys()))\n",
        "print(\"Precipitations: {}\".format(nc_prec.dimensions.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precipitation variable: <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 prAdjust(time, y, x)\n",
            "    _FillValue: 1e+20\n",
            "    missing_value: 1e+20\n",
            "    cell_measures: area:areacella\n",
            "    cell_methods: time:mean\n",
            "    long_name: precipitation\n",
            "    standard_name: mean_precipitation_index_per_time_period\n",
            "    units: kg m-2 s-1\n",
            "    variable: prAdjust\n",
            "    coordinates: lat lon\n",
            "unlimited dimensions: time\n",
            "current shape = (365, 950, 1000)\n",
            "filling on\n"
          ]
        }
      ],
      "source": [
        "print('Precipitation variable: {}'.format(nc_prec['prAdjust']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The precipitation variables have three dimensions, like river discharges. The current shape is similar to the RDH. However, we need to analyze the current shape rotation for correct geographical references during the measurements comparisons and data aggregation with RDH. Measurements are in $mm$ per $ m^2/s$; the current bound is limited for the point projection and unlimited for the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature variable: <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 tasAdjust(time, y, x)\n",
            "    _FillValue: 1e+20\n",
            "    missing_value: 1e+20\n",
            "    cell_measures: area:areacella\n",
            "    cell_methods: time:mean\n",
            "    long_name: mean air temperature\n",
            "    standard_name: mean_temperature_index_per_time_period\n",
            "    units: K\n",
            "    variable: tasAdjust\n",
            "    coordinates: lat lon height\n",
            "unlimited dimensions: time\n",
            "current shape = (365, 950, 1000)\n",
            "filling on\n"
          ]
        }
      ],
      "source": [
        "print('Temperature variable: {}'.format(nc_temp['tasAdjust']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The temperature shape is the same as the precipitation, and this is good information about the integration between precipitations and temperatures on geographical points. On the standard data definition and metadata descriptions, temperatures and precipitations are a mean on a specific geographical area corresponding to a specific point. So, we can smooth the variance of temperatures and precipitations around a specific point with their mean directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison between RDH and TPI on coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the exploratory on RDH and TPI and from the dataset description on the CDS, we saw how data are distributed and which dimensions they are. Furthermore, we saw that the size for each dimension is the same over time, but projections coordinates are not. We need to find a function $f: f(x,y) = (x', y') : x,y \\in RDH \\land x',y' \\in TPI$ or vice-versa. According to the official datasets documentation, each measurement on TPI and RDH are values related to the same dimension time (one time per day at 12:00 a.m. for each day in a year). In conclusion, we can analyze the main features for each measurement in the same way and compares possible results as additional information which should be helpful for critical decision during data management. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Temperatures variables size for 1 year: \n",
            "Size latitude: 950\n",
            "Size longitude: 950 \n",
            "Size time for each point: 365\n",
            "--------------------\n",
            "Temperatures variables size for 1 year: \n",
            "Size latitude: 950\n",
            "Size longitude: 950 \n",
            "Size time for each point: 365\n",
            "--------------------\n",
            "Discharges variables size for 2 year: \n",
            "Size latitude: 950\n",
            "Size longitude: 950 \n",
            "Size time for each point: 498\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"-\"*20)\n",
        "print('Temperatures variables size for 1 year: ')\n",
        "print(\"Size latitude: {}\".format(len(nc_temp.variables['lat'][:][:])))\n",
        "print(\"Size longitude: {} \".format(len(nc_temp.variables['lon'][:][:])))\n",
        "print(\"Size time for each point: {}\".format(len(nc_temp.variables['time'][:][:])))\n",
        "\n",
        "print(\"-\"*20)\n",
        "print('Temperatures variables size for 1 year: ')\n",
        "print(\"Size latitude: {}\".format(len(nc_prec.variables['lat'][:][:])))\n",
        "print(\"Size longitude: {} \".format(len(nc_prec.variables['lon'][:][:])))\n",
        "print(\"Size time for each point: {}\".format(len(nc_prec.variables['time'][:][:])))\n",
        "\n",
        "print(\"-\"*20)\n",
        "print('Discharges variables size for 2 year: ')\n",
        "print(\"Size latitude: {}\".format(len(nc_dis.variables['latitude'][:][:])))\n",
        "print(\"Size longitude: {} \".format(len(nc_dis.variables['longitude'][:][:])))\n",
        "print(\"Size time for each point: {}\".format(len(nc_dis.variables['time'][:][:])))\n",
        "print(\"-\"*20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The size of the time on a point for the discharge data is only $489$ because 2022 is the present, and Copernicus provides only data without predictions.\n",
        "\n",
        "Let us now analyze how geographical latitude and longitude are related between TPI and RDH. The scope of the following analysis is to explore location distributions and find possible data correlations on geographical references between TPI and RDH measurements. The result of this phase is a function that maps TPI measurements to the same point of RDH values considering latitude and longitude pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables Precipitations: dict_keys(['time', 'lon', 'lat', 'prAdjust'])\n",
            "Variables Temperatures: dict_keys(['time', 'lon', 'lat', 'tasAdjust', 'height'])\n"
          ]
        }
      ],
      "source": [
        "print('Variables Precipitations: {}'.format(nc_prec.variables.keys()))\n",
        "print('Variables Temperatures: {}'.format(nc_temp.variables.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the precipitations and temperatures on a same location over time:\n",
            "Precipitations: 365\n",
            "Temperatures: 365\n"
          ]
        }
      ],
      "source": [
        "print('Size of the precipitations and temperatures on a same location over time:')\n",
        "print(\"Precipitations: {}\".format(len(nc_prec.variables['prAdjust'][:][:][:])))\n",
        "print(\"Temperatures: {}\".format(len(nc_temp.variables['tasAdjust'][:][:][:])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the same amount of measurement for temperatures and precipitations over time, and the only observation for this pair of values is coordinated possible mismatches. First of all, we need to check if the geographical correlation focuses on an identity function between precipitations and temperatures:\n",
        "\n",
        "$f(x,y) = (x',y') : x = x' \\land y = y'\\ \\forall\\ x,y \\in T\\ \\land \\forall\\ x',y' \\in P$ with $T$ the set of temperatures measurements and $P$ the precipitations values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "size_prec_lon = len(nc_prec.variables['lon'][:][:][:]) \n",
        "size_temp_lon = len(nc_prec.variables['lon'][:][:][:]) \n",
        "size_prec_lat = len(nc_prec.variables['lat'][:][:][:]) \n",
        "size_temp_lat = len(nc_prec.variables['lat'][:][:][:]) \n",
        "if size_prec_lon != size_temp_lon or size_prec_lat != size_temp_lat:\n",
        "    print('Geographical references missmatch on the number of locations.')\n",
        "\n",
        "flag = True\n",
        "x = 950\n",
        "y = 1000\n",
        "for i in range(x):\n",
        "    # on y constant \n",
        "    lat_prec = nc_prec.variables['lat'][i][0] \n",
        "    lat_temp = nc_temp.variables['lat'][i][0] \n",
        "    lon_prec = nc_prec.variables['lon'][i][0]\n",
        "    lon_temp = nc_temp.variables['lon'][i][0]\n",
        "    if(lat_prec != lat_temp or lon_prec != lon_temp):\n",
        "        flag = False\n",
        "        break\n",
        "\n",
        "for j in range(y):\n",
        "    # on x constant \n",
        "    lat_prec = nc_prec.variables['lat'][0][j] \n",
        "    lat_temp = nc_temp.variables['lat'][0][j] \n",
        "    lon_prec = nc_prec.variables['lon'][0][j]\n",
        "    lon_temp = nc_temp.variables['lon'][0][j]\n",
        "    if(lat_prec != lat_temp or lon_prec != lon_temp):\n",
        "        flag = False\n",
        "        break\n",
        "\n",
        "print(flag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given $(time, x, y)$, we can test only on one dimension over time. So, we put x and y sequentially constant on multiple controls. The output confirms that the correlation between coordinates on spatial dimensions focuses on the identity function. In other words, we have the same references on latitude and longitude for each measurement on the dataset over time. This notice is suitable for data management and aggregation because we can directly merge temperature and precipitations on the exact coordinates over time dimensions. \n",
        "\n",
        "Finally, we can analyze the possible correlation on geographical references between TPI and RDH measurements, which is the last part that we didn't have yet. Thanks to the identity function on the correlation between temperatures and precipitations for spatial dimensions, we can compare measurements references only on one of these two values with the river discharge monitoring. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "size_prec_lon = len(nc_prec.variables['lon'][:][:][:]) \n",
        "size_dis_lon = len(nc_dis.variables['longitude'][:][:][:]) \n",
        "size_prec_lat = len(nc_prec.variables['lat'][:][:][:]) \n",
        "size_dis_lat = len(nc_dis.variables['latitude'][:][:][:]) \n",
        "if size_prec_lon != size_dis_lon or size_prec_lat != size_dis_lat:\n",
        "    print('Geographical references missmatch on the number of locations.')\n",
        "\n",
        "flag = True\n",
        "x = 950\n",
        "y = 1000\n",
        "for i in range(x):\n",
        "    # on y constant \n",
        "    lat_prec = nc_prec.variables['lat'][i][0] \n",
        "    lat_dis = nc_dis.variables['latitude'][i][0] \n",
        "    lon_prec = nc_prec.variables['lon'][i][0]\n",
        "    lon_dis = nc_dis.variables['longitude'][i][0]\n",
        "    if(lat_prec != lat_dis or lon_prec != lon_dis):\n",
        "        flag = False\n",
        "        break\n",
        "\n",
        "for j in range(y):\n",
        "    # on x constant \n",
        "    lat_prec = nc_prec.variables['lat'][0][j] \n",
        "    lat_dis = nc_dis.variables['latitude'][0][j] \n",
        "    lon_prec = nc_prec.variables['lon'][0][j]\n",
        "    lon_dis = nc_dis.variables['longitude'][0][j]\n",
        "    if(lat_prec != lat_dis or lon_prec != lon_dis):\n",
        "        flag = False\n",
        "        break\n",
        "\n",
        "print(flag)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, TPI and RDH didn't have a spatial correlation equal to the identity functions as precipitations and temperatures do. So, we need to find some spatial referent on latitude and longitude and find a possible process to map TPI locations on RDH locations and aggregate their measurements. We will test only on some samples, and if we have a positive result, we will extend verification to the entire domain. Our hypothesis is related to possible rotations on coordinates on the x and y dimensions because we had positive feedback on the spatial dimensions size comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f(x,y) = (-x, y) hypothesis possible\n"
          ]
        }
      ],
      "source": [
        "if nc_prec.variables['lat'][0][0] == nc_dis.variables['latitude'][949][949]:\n",
        "    print(\"f(x,y) = (-x,-y) hypothesis possible\")\n",
        "\n",
        "if nc_prec.variables['lat'][0][0] == nc_dis.variables['latitude'][949][0]:\n",
        "    print(\"f(x,y) = (-x, y) hypothesis possible\")\n",
        "\n",
        "if nc_prec.variables['lat'][0][0] == nc_dis.variables['latitude'][0][949]:\n",
        "    print(\"f(x,y) = (x, -y) hypothesis possible\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We tested every possible rotation function on the spatial matrix (x,y). The identity function ($f(x,y) = (x,y)$) failed on the first hypothesis described in the previous snippet. Due to the uniform distribution over time of TPI and RDH, we can verify or refuse the mathematical hypothesis on the correlation between coordinates of TPI and RDH considering only our subsample dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "flag = True\n",
        "x = 950\n",
        "y = 1000\n",
        "for i in range(x):\n",
        "    lat_prec = nc_prec.variables['lat'][i][0] \n",
        "    lat_dis = nc_dis.variables['latitude'][x-i-1][0] \n",
        "    lon_prec = nc_prec.variables['lon'][i][0]\n",
        "    lon_dis = nc_dis.variables['longitude'][x-i-1][0]\n",
        "    if(lat_prec != lat_dis or lon_prec != lon_dis):\n",
        "        flag = False\n",
        "        break\n",
        "\n",
        "print(flag)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0,10) : 27.906766891479492 <-> (949,10) : 27.906766891479492\n",
            "(1,10) : 27.951608657836914 <-> (948,10) : 27.951608657836914\n",
            "(2,10) : 27.996444702148438 <-> (947,10) : 27.996444702148438\n",
            "(3,10) : 28.041275024414062 <-> (946,10) : 28.041275024414062\n",
            "(4,10) : 28.08609962463379 <-> (945,10) : 28.08609962463379\n",
            "(5,10) : 28.130918502807617 <-> (944,10) : 28.130918502807617\n",
            "(6,10) : 28.175731658935547 <-> (943,10) : 28.175731658935547\n",
            "(7,10) : 28.220539093017578 <-> (942,10) : 28.220539093017578\n",
            "(8,10) : 28.265338897705078 <-> (941,10) : 28.265338897705078\n",
            "(9,10) : 28.310134887695312 <-> (940,10) : 28.310134887695312\n"
          ]
        }
      ],
      "source": [
        "# latitude and longitude are 950 for a specific unit of time, this control returns some \n",
        "for i in range(0,10):\n",
        "    print(\"({},{}) : {} <-> ({},{}) : {}\".format(i, 10, nc_prec.variables['lat'][i][10], 949-i, 10, nc_dis.variables['latitude'][949-i][10])) # f(i,j) == (-i, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, the initial exploratory of spatial data retrieves some essential features about measurement distributions, size, and dimensions of data, multi-views analysis, and metadata extrapolations. These kinds of information should be helpful in data aggregation over the time dimension and facilitate some data manipulations due to the discovery of the rotation functions on the spatial matrix of TPI on RDH."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Initial Exploratory Spatial Data Analysis",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
